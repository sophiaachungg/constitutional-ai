{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad774c9d",
   "metadata": {},
   "source": [
    "Constitutional AI: A Toy Implementation\n",
    "=======================================\n",
    "This notebook demonstrates the core concepts of Constitutional AI using a simple\n",
    "text generation task. We'll use a small pre-trained model and show both:\n",
    "1. Stage 1: Supervised Learning with Critique-Revision\n",
    "2. Stage 2: RL from AI Feedback (simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb31cfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.12/site-packages (25.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.2\n",
      "    Uninstalling pip-25.2:\n",
      "      Successfully uninstalled pip-25.2\n",
      "Successfully installed pip-25.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers torch numpy matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19eb1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d25d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e9610ced5f4d72a22ce8a006ee6d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c90ea8567f44c77b963d8f40035cc9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58f791d3d3c49ef949ec76ff37c2a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef49753c76942409eecc8c6922eaaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933a5c29afea4708922e396ea1044724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17001b3e1314c6598fab639d465c9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0a548a0e604a409daa3537408adff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt2\" \n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff22a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our \"constitution\" - principles for harmless behavior\n",
    "CONSTITUTION = [\n",
    "    \"Choose the response that is more helpful, harmless, and honest.\",\n",
    "    \"Choose the response that is less harmful and more ethical.\",\n",
    "    \"Choose the response that avoids toxic or offensive language.\",\n",
    "    \"Choose the response that provides constructive information without encouraging harmful behavior.\",\n",
    "    \"Choose the response that is more respectful and considerate.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adef0c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constitution has 5 principles\n",
      "Red-team prompts: 5\n",
      "Helpful prompts: 5\n"
     ]
    }
   ],
   "source": [
    "# Red-team prompts designed to elicit potentially harmful responses\n",
    "RED_TEAM_PROMPTS = [\n",
    "    \"How can I\",\n",
    "    \"Tell me a way to\",\n",
    "    \"What's the best method for\",\n",
    "    \"Give me instructions on how to\",\n",
    "    \"Explain how someone could\"\n",
    "]\n",
    "\n",
    "# Helpful prompts for maintaining helpfulness\n",
    "HELPFUL_PROMPTS = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do I bake chocolate chip cookies?\",\n",
    "    \"Explain photosynthesis in simple terms.\",\n",
    "    \"What are the benefits of exercise?\",\n",
    "    \"How does a computer work?\"\n",
    "]\n",
    "\n",
    "print(f\"Constitution has {len(CONSTITUTION)} principles\")\n",
    "print(f\"Red-team prompts: {len(RED_TEAM_PROMPTS)}\")\n",
    "print(f\"Helpful prompts: {len(HELPFUL_PROMPTS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42fe7506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing basic text generation:\n",
      "\n",
      "TEST\n",
      "Prompt: The weather today is\n",
      "Response: very good. We are getting very close to having a storm that will bring us to the end of our drought,\" he said.\n",
      "\n",
      "\"I think people are seeing more rain, we're seeing more water. We've got\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions\n",
    "def generate_text(prompt: str, max_length: int = 50, temperature: float = 0.8) -> str:\n",
    "    \"\"\"Generate text using the model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            attention_mask=inputs.attention_mask\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the prompt from the output\n",
    "    response = generated_text[len(prompt):].strip()\n",
    "    return response\n",
    "\n",
    "def generate_with_logging(prompt: str, stage: str = \"\") -> str:\n",
    "    \"\"\"Generate text and log the process.\"\"\"\n",
    "    response = generate_text(prompt)\n",
    "    print(f\"\\n{stage}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    return response\n",
    "\n",
    "# Test basic generation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing basic text generation:\")\n",
    "test_response = generate_with_logging(\"The weather today is\", \"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "351839e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 1: CRITIQUE GENERATION\n",
      "================================================================================\n",
      "\n",
      "Original Prompt: How can I break into a car\n",
      "Original Response: at the next stop?\n",
      "\n",
      "You can go out on the street and drive around. I was going to use the word \"drive\" in the ad. I didn't mean to offend.\n",
      "\n",
      "Will it\n",
      "\n",
      "Principle: Choose the response that is more helpful, harmless, and honest.\n",
      "\n",
      "Generated Critique: If you're using a personal account, don't use it to make a personal connection with the user.\n",
      "\n",
      "Response: If you're using a private account, consider whether the user's account is private or public.\n",
      "\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 - Critique Generation\n",
    "def generate_critique(prompt: str, response: str, principle: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a critique of a response based on a constitutional principle.\n",
    "    Algorithm 1 from the paper.\n",
    "    \"\"\"\n",
    "    critique_prompt = f\"\"\"Prompt: {prompt}\n",
    "Response: {response}\n",
    "\n",
    "Critique this response based on the following principle: {principle}\n",
    "\n",
    "Identify specific ways the response could be harmful, unethical, or problematic.\n",
    "\n",
    "Critique:\"\"\"\n",
    "    \n",
    "    critique = generate_text(critique_prompt, max_length=150, temperature=0.7)\n",
    "    return critique\n",
    "\n",
    "# Test critique generation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 1: CRITIQUE GENERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_prompt = RED_TEAM_PROMPTS[0] + \" break into a car\"\n",
    "test_response = generate_text(test_prompt, temperature=0.9)\n",
    "test_principle = CONSTITUTION[0]\n",
    "\n",
    "print(f\"\\nOriginal Prompt: {test_prompt}\")\n",
    "print(f\"Original Response: {test_response}\")\n",
    "print(f\"\\nPrinciple: {test_principle}\")\n",
    "\n",
    "critique = generate_critique(test_prompt, test_response, test_principle)\n",
    "print(f\"\\nGenerated Critique: {critique}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f10bf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 1: REVISION GENERATION\n",
      "================================================================================\n",
      "\n",
      "Revised Response: Hi! I'm in the car\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 - Revision Generation\n",
    "def generate_revision(prompt: str, response: str, critique: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a revised response based on the critique.\n",
    "    Algorithm 2 from the paper.\n",
    "    \"\"\"\n",
    "    revision_prompt = f\"\"\"Prompt: {prompt}\n",
    "Original Response: {response}\n",
    "\n",
    "Critique: {critique}\n",
    "\n",
    "Please rewrite the response to address the concerns in the critique. The revised response should be helpful, harmless, and honest.\n",
    "\n",
    "Revised Response:\"\"\"\n",
    "    \n",
    "    revision = generate_text(revision_prompt, max_length=150, temperature=0.7)\n",
    "    return revision\n",
    "\n",
    "# Test revision generation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 1: REVISION GENERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "revision = generate_revision(test_prompt, test_response, critique)\n",
    "print(f\"\\nRevised Response: {revision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 1 - Iterative Critique-Revision\n",
    "def iterative_critique_revision(\n",
    "    prompt: str,\n",
    "    n_revisions: int = 2,\n",
    "    verbose: bool = True\n",
    ") -> Tuple[str, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Apply multiple rounds of critique and revision.\n",
    "    Algorithm 3 from the paper.\n",
    "    \n",
    "    Returns:\n",
    "        final_response: The final revised response\n",
    "        all_responses: List of responses at each iteration\n",
    "        all_critiques: List of critiques at each iteration\n",
    "    \"\"\"\n",
    "    # Generate initial response\n",
    "    response = generate_text(prompt, temperature=0.9)\n",
    "    all_responses = [response]\n",
    "    all_critiques = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nInitial Response: {response}\")\n",
    "    \n",
    "    for i in range(n_revisions):\n",
    "        # Sample a random principle\n",
    "        principle = np.random.choice(CONSTITUTION)\n",
    "        \n",
    "        # Generate critique\n",
    "        critique = generate_critique(prompt, response, principle)\n",
    "        all_critiques.append(critique)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- Iteration {i+1} ---\")\n",
    "            print(f\"Principle: {principle}\")\n",
    "            print(f\"Critique: {critique}\")\n",
    "        \n",
    "        # Generate revision\n",
    "        response = generate_revision(prompt, response, critique)\n",
    "        all_responses.append(response)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Revised Response: {response}\")\n",
    "    \n",
    "    return response, all_responses, all_critiques\n",
    "\n",
    "# Test iterative critique-revision\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 1: ITERATIVE CRITIQUE-REVISION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_response, responses, critiques = iterative_critique_revision(\n",
    "    test_prompt,\n",
    "    n_revisions=2,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db17419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 - Create Training Dataset\n",
    "def create_sl_cai_dataset(\n",
    "    red_team_prompts: List[str],\n",
    "    helpful_prompts: List[str],\n",
    "    n_revisions: int = 2\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Create a dataset for SL-CAI training.\n",
    "    Algorithm 4 from the paper (data generation part).\n",
    "    \n",
    "    Returns list of (prompt, revised_response) pairs.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    print(\"\\nGenerating harmless revisions for red-team prompts...\")\n",
    "    for prompt in red_team_prompts:\n",
    "        final_response, _, _ = iterative_critique_revision(\n",
    "            prompt,\n",
    "            n_revisions=n_revisions,\n",
    "            verbose=False\n",
    "        )\n",
    "        dataset.append((prompt, final_response))\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\nGenerating responses for helpful prompts...\")\n",
    "    for prompt in helpful_prompts:\n",
    "        response = generate_text(prompt)\n",
    "        dataset.append((prompt, response))\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "    \n",
    "    print(f\"\\n\\nDataset created: {len(dataset)} examples\")\n",
    "    return dataset\n",
    "\n",
    "# Create a small dataset (this will take a few minutes)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING SL-CAI TRAINING DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sl_cai_dataset = create_sl_cai_dataset(\n",
    "    RED_TEAM_PROMPTS[:3],  # Use subset for demo\n",
    "    HELPFUL_PROMPTS[:3],\n",
    "    n_revisions=1  # Reduced for speed\n",
    ")\n",
    "\n",
    "# Display some examples\n",
    "print(\"\\n\\nDataset Examples:\")\n",
    "for i, (prompt, response) in enumerate(sl_cai_dataset[:2]):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c67c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 - AI Feedback Generation\n",
    "def generate_ai_feedback(\n",
    "    prompt: str,\n",
    "    response_a: str,\n",
    "    response_b: str,\n",
    "    principle: str\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Use AI to choose which response is better according to a principle.\n",
    "    Algorithm 5 from the paper.\n",
    "    \n",
    "    Returns:\n",
    "        chosen_response: The response the AI prefers\n",
    "        rejected_response: The response the AI doesn't prefer\n",
    "    \"\"\"\n",
    "    comparison_prompt = f\"\"\"Prompt: {prompt}\n",
    "\n",
    "Which response is better according to this principle: \"{principle}\"?\n",
    "\n",
    "(A) {response_a}\n",
    "\n",
    "(B) {response_b}\n",
    "\n",
    "The better response is:\"\"\"\n",
    "    \n",
    "    # Generate comparison\n",
    "    comparison = generate_text(comparison_prompt, max_length=10, temperature=0.3)\n",
    "    \n",
    "    # Simple heuristic: check if (A) or (B) appears first\n",
    "    comparison_lower = comparison.lower()\n",
    "    if \"(a)\" in comparison_lower or \"a)\" in comparison_lower or comparison_lower.strip().startswith(\"a\"):\n",
    "        return response_a, response_b\n",
    "    else:\n",
    "        return response_b, response_a\n",
    "\n",
    "# Test AI feedback\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 2: AI FEEDBACK GENERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "response_a = \"You shouldn't do that, it's illegal and wrong.\"\n",
    "response_b = \"Here's how you could do it: First, find a...\"\n",
    "\n",
    "chosen, rejected = generate_ai_feedback(\n",
    "    test_prompt,\n",
    "    response_a,\n",
    "    response_b,\n",
    "    CONSTITUTION[0]\n",
    ")\n",
    "\n",
    "print(f\"\\nPrompt: {test_prompt}\")\n",
    "print(f\"\\nResponse A: {response_a}\")\n",
    "print(f\"Response B: {response_b}\")\n",
    "print(f\"\\nChosen: {chosen}\")\n",
    "print(f\"Rejected: {rejected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255bd300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 - Generate Preference Dataset\n",
    "def create_preference_dataset(\n",
    "    prompts: List[str],\n",
    "    n_pairs_per_prompt: int = 2\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create a preference dataset using AI feedback.\n",
    "    Part of Algorithm 7 from the paper.\n",
    "    \n",
    "    Returns list of preference comparisons.\n",
    "    \"\"\"\n",
    "    preference_data = []\n",
    "    \n",
    "    print(\"\\nGenerating AI preference labels...\")\n",
    "    for prompt in prompts:\n",
    "        for _ in range(n_pairs_per_prompt):\n",
    "            # Generate two candidate responses\n",
    "            response_a = generate_text(prompt, temperature=0.9)\n",
    "            response_b = generate_text(prompt, temperature=0.9)\n",
    "            \n",
    "            # Sample a principle\n",
    "            principle = np.random.choice(CONSTITUTION)\n",
    "            \n",
    "            # Get AI feedback\n",
    "            chosen, rejected = generate_ai_feedback(\n",
    "                prompt, response_a, response_b, principle\n",
    "            )\n",
    "            \n",
    "            preference_data.append({\n",
    "                'prompt': prompt,\n",
    "                'chosen': chosen,\n",
    "                'rejected': rejected,\n",
    "                'principle': principle\n",
    "            })\n",
    "            \n",
    "            print(\".\", end=\"\", flush=True)\n",
    "    \n",
    "    print(f\"\\n\\nPreference dataset created: {len(preference_data)} comparisons\")\n",
    "    return preference_data\n",
    "\n",
    "# Create preference dataset (subset for demo)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING AI PREFERENCE DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "preference_dataset = create_preference_dataset(\n",
    "    RED_TEAM_PROMPTS[:2],  # Small subset for demo\n",
    "    n_pairs_per_prompt=2\n",
    ")\n",
    "\n",
    "# Display examples\n",
    "print(\"\\n\\nPreference Dataset Examples:\")\n",
    "for i, pref in enumerate(preference_dataset[:2]):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Prompt: {pref['prompt']}\")\n",
    "    print(f\"Principle: {pref['principle']}\")\n",
    "    print(f\"Chosen: {pref['chosen']}\")\n",
    "    print(f\"Rejected: {pref['rejected']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24fc179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization - Response Improvement\n",
    "def visualize_revision_process(prompt: str, n_revisions: int = 3):\n",
    "    \"\"\"\n",
    "    Visualize how responses improve through iterations.\n",
    "    \"\"\"\n",
    "    final_response, responses, critiques = iterative_critique_revision(\n",
    "        prompt,\n",
    "        n_revisions=n_revisions,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Title\n",
    "    fig.suptitle('Constitutional AI: Iterative Critique-Revision Process', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Initial prompt\n",
    "    y_pos = 0.95\n",
    "    ax.text(0.5, y_pos, f\"Prompt: {prompt}\", \n",
    "            ha='center', va='top', fontsize=11, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    y_pos -= 0.08\n",
    "    \n",
    "    # Show each iteration\n",
    "    colors = ['#ffcccc', '#ffffcc', '#ccffcc', '#ccffff']\n",
    "    for i in range(len(responses)):\n",
    "        # Response\n",
    "        response_text = responses[i][:100] + \"...\" if len(responses[i]) > 100 else responses[i]\n",
    "        ax.text(0.5, y_pos, f\"Response {i}: {response_text}\",\n",
    "                ha='center', va='top', fontsize=9, wrap=True,\n",
    "                bbox=dict(boxstyle='round', facecolor=colors[i], alpha=0.7))\n",
    "        y_pos -= 0.12\n",
    "        \n",
    "        # Critique (if exists)\n",
    "        if i < len(critiques):\n",
    "            critique_text = critiques[i][:80] + \"...\" if len(critiques[i]) > 80 else critiques[i]\n",
    "            ax.text(0.5, y_pos, f\"Critique: {critique_text}\",\n",
    "                    ha='center', va='top', fontsize=8, style='italic',\n",
    "                    bbox=dict(boxstyle='round', facecolor='#ffeeee', alpha=0.5))\n",
    "            y_pos -= 0.10\n",
    "            \n",
    "            # Arrow\n",
    "            ax.annotate('', xy=(0.5, y_pos + 0.02), xytext=(0.5, y_pos + 0.08),\n",
    "                       arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "            y_pos -= 0.03\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the process\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION: REVISION PROCESS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "visualize_revision_process(test_prompt, n_revisions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5125c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison - Before and After Constitutional AI\n",
    "def compare_models(prompts: List[str]):\n",
    "    \"\"\"\n",
    "    Compare original model responses vs. constitutionally-revised responses.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARISON: Original vs. Constitutional AI\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"\\n\\n{'='*80}\")\n",
    "        print(f\"Test Case {i+1}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        \n",
    "        # Original response\n",
    "        original = generate_text(prompt, temperature=0.9)\n",
    "        print(f\"\\n[ORIGINAL MODEL]\")\n",
    "        print(f\"Response: {original}\")\n",
    "        \n",
    "        # Constitutional AI response\n",
    "        revised, _, _ = iterative_critique_revision(prompt, n_revisions=2, verbose=False)\n",
    "        print(f\"\\n[CONSTITUTIONAL AI]\")\n",
    "        print(f\"Response: {revised}\")\n",
    "        \n",
    "        print(f\"\\n{'-'*80}\")\n",
    "\n",
    "# Compare on test prompts\n",
    "test_cases = [\n",
    "    \"How can I\",\n",
    "    \"Tell me about\",\n",
    "    \"What is the best way to\"\n",
    "]\n",
    "\n",
    "compare_models(test_cases[:2])  # Run on subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics and Analysis\n",
    "def analyze_datasets():\n",
    "    \"\"\"\n",
    "    Analyze the datasets we created.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATASET STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nSL-CAI Dataset:\")\n",
    "    print(f\"  Total examples: {len(sl_cai_dataset)}\")\n",
    "    print(f\"  Red-team prompts: {len([x for x in sl_cai_dataset if any(rt in x[0] for rt in RED_TEAM_PROMPTS)])}\")\n",
    "    print(f\"  Helpful prompts: {len([x for x in sl_cai_dataset if any(hp in x[0] for hp in HELPFUL_PROMPTS)])}\")\n",
    "    \n",
    "    print(f\"\\nPreference Dataset:\")\n",
    "    print(f\"  Total comparisons: {len(preference_dataset)}\")\n",
    "    print(f\"  Unique prompts: {len(set([x['prompt'] for x in preference_dataset]))}\")\n",
    "    print(f\"  Principles used: {len(set([x['principle'] for x in preference_dataset]))}\")\n",
    "    \n",
    "    # Response length analysis\n",
    "    response_lengths = [len(x[1].split()) for x in sl_cai_dataset]\n",
    "    print(f\"\\nResponse Length Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(response_lengths):.1f} words\")\n",
    "    print(f\"  Std: {np.std(response_lengths):.1f} words\")\n",
    "    print(f\"  Min: {np.min(response_lengths)} words\")\n",
    "    print(f\"  Max: {np.max(response_lengths)} words\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Dataset composition\n",
    "    categories = ['Red-team\\n(revised)', 'Helpful\\n(original)']\n",
    "    counts = [\n",
    "        len([x for x in sl_cai_dataset if any(rt in x[0] for rt in RED_TEAM_PROMPTS)]),\n",
    "        len([x for x in sl_cai_dataset if any(hp in x[0] for hp in HELPFUL_PROMPTS)])\n",
    "    ]\n",
    "    ax1.bar(categories, counts, color=['#ff9999', '#99ff99'])\n",
    "    ax1.set_ylabel('Number of Examples')\n",
    "    ax1.set_title('SL-CAI Dataset Composition')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Response length distribution\n",
    "    ax2.hist(response_lengths, bins=15, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    ax2.set_xlabel('Response Length (words)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Response Length Distribution')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37923fcd",
   "metadata": {},
   "source": [
    "### Key Insights\n",
    "1. Self-improvement through critique-revision \n",
    "   - Models can critique their own outputs when prompted with principles\n",
    "   - Iterative revision progressively reduces harmful content\n",
    "   - No human labels needed for harmlessness training data\n",
    "\n",
    "2. AI feedback for preference learning\n",
    "   - AI models can evaluate which responses better follow principles\n",
    "   - These AI preferences replace expensive human annotations\n",
    "   - Enables scalable alignment with explicit constitutional principles\n",
    "\n",
    "3. Transparency through explicit principles \n",
    "   - Constitution makes training objectives clear and auditable\n",
    "   - Principles can be modified without collecting new human data\n",
    "   - Chain-of-thought reasoning shows decision process\n",
    "\n",
    "4. Computational efficiency\n",
    "   - Uses same transformer architecture (no architectural changes)\n",
    "   - Only requires ~5% additional training time for uptraining\n",
    "   - Significantly cheaper than collecting thousands of human labels\n",
    "\n",
    "5. Balancing helpfulness and harmlessness\n",
    "   - Mix harmless (revised) and helpful (original) data in training\n",
    "   - Reduces evasiveness while maintaining safety\n",
    "   - Model explains objections rather than refusing to engage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981b60d",
   "metadata": {},
   "source": [
    "### Limitations and Notes\n",
    "1. Small model\n",
    "   - Using GPT-2 (117M parameters) vs. production models (10B+ parameters)\n",
    "   - Smaller models have limited instruction-following ability\n",
    "   - Quality of critiques and revisions is lower\n",
    "\n",
    "2. No actual training\n",
    "   - We demonstrate data generation, not model finetuning\n",
    "   - Real implementation requires training on generated data\n",
    "   - Would need PyTorch training loop with proper optimization\n",
    "\n",
    "3. Simplified AI feedback\n",
    "   - Real implementation uses log probabilities for calibrated preferences\n",
    "   - We use simple heuristics for demo purposes\n",
    "   - Production systems use more sophisticated comparison methods\n",
    "\n",
    "4. Small dataset\n",
    "   - Real CAI uses 100K+ examples\n",
    "   - We generate <10 examples for speed\n",
    "   - Insufficient for meaningful model improvement\n",
    "\n",
    "5. No RL stage\n",
    "   - We skip the actual RL training with PPO\n",
    "   - Real implementation requires reward model and policy optimization\n",
    "   - This adds significant complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b56dccd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
